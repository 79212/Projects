{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_Frequency_JP.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1lH7HS-zi_4RxR-NXkYgLCooedw6O90Z4","authorship_tag":"ABX9TyOgaQ0bMOSrH44ZzLsZb7O0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"F1XdhS1S_JVP","colab_type":"text"},"source":["##Import Libraries"]},{"cell_type":"code","metadata":{"id":"yptE8pntMjiV","colab_type":"code","colab":{}},"source":["!pip install spacy\n","!pip install pysrt #per sottotitoli"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MN94ljgCp-dQ","colab_type":"code","colab":{}},"source":["#find top words\n","def top_frequent(text, num_words):\n","  #frequency of most common words\n","  import spacy\n","  from collections import Counter\n","\n","  nlp = spacy.load(\"en\")\n","  text = text\n","\n","  #lemmatization\n","  doc = nlp(text)\n","  token_list = list()\n","  for token in doc:\n","    #print(token, token.lemma_)\n","    token_list.append(token.lemma_)\n","  token_list\n","\n","  lemmatized = ''\n","  for _ in token_list:\n","    lemmatized = lemmatized + ' ' + _\n","  lemmatized\n","\n","  #remove stopwords and punctuations\n","  doc = nlp(lemmatized)\n","  words = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n","  word_freq = Counter(words)\n","  common_words = word_freq.most_common(num_words)\n","  return common_words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kC-N5nnG_DIk","colab_type":"text"},"source":["##Total Course"]},{"cell_type":"code","metadata":{"id":"UWWhoewLPNtb","colab_type":"code","colab":{}},"source":["#   parse sub\n","#total words of JP in the course\n","import pysrt\n","\n","total_string = ''\n","for text_n in range(1, 10):\n","  subs = pysrt.open('/content/drive/My Drive/Colab Notebooks/Projects/20200729_NLP_frequency/JP_'+str(text_n)+'.srt')\n","  for sub in subs:\n","    #print(sub.text)\n","    total_string += sub.text\n","total_string"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PogZS2SUvzlI","colab_type":"code","colab":{}},"source":["len(total_string)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Acg1otqzSVRG","colab_type":"code","colab":{}},"source":["common_words = top_frequent(total_string, 1000)\n","common_words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"24LKswpw_GeK","colab_type":"text"},"source":["##Individual Lessons"]},{"cell_type":"code","metadata":{"id":"k2nXwlmuX4WS","colab_type":"code","colab":{}},"source":["#https://www.twinword.com/api/language-scoring.php\n","#Language Scoring"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T9w0F5WNijsy","colab_type":"code","colab":{}},"source":["def find_common_factors(X):\n","  #flat everything in one list\n","  one_row = list()\n","  for k in X.values:\n","    for n in k:\n","      one_row.append(n)\n","  one_row = pd.DataFrame(one_row)\n","  one_row\n","\n","  #labeled list\n","  from sklearn.preprocessing import LabelEncoder\n","  le = LabelEncoder()\n","  le.fit(one_row[0])\n","  one_row_labeled = le.transform(one_row[0])\n","  one_row_labeled\n","\n","  #rebuild the original dataset and convert it to DataFrame\n","  import numpy\n","  X_labeled = numpy.array_split(one_row_labeled, (X.shape[0]))\n","  X_labeled = pd.DataFrame(X_labeled)\n","  X_labeled\n","\n","  m = [[0 for x in range(max(one_row_labeled)+1)] for x in range(len(X_labeled))]\n","\n","  #turn each corresponding label to 1\n","  for row in range(len(X_labeled.values)):\n","    for num in range(len(X_labeled.values[1])):\n","      m[row][X_labeled.values[row][num]] = 1\n","  m = pd.DataFrame(m)\n","  m\n","\n","  original_shape = m.shape\n","  original_shape\n","\n","  #convert column num in corresponding text\n","  m.loc[original_shape[0]] = [0 for x in range(0, original_shape[1])]\n","  m\n","\n","  for _ in range(original_shape[0]):\n","    m.loc[original_shape[0]] += m.loc[_] \n","  m\n","\n","  #rename columns with label values\n","  m.columns = a = le.inverse_transform(m.columns)\n","  m\n","\n","  #isolate count of columns\n","  m.loc[m.shape[0]] = m.columns #we add last col with names\n","  f = m.transpose().drop([x for x in range(m.shape[0]-2)], axis=1).transpose() #we only save the last 2 cols\n","  f = f.reset_index()\n","  f.pop('index')\n","\n","  #sorting values\n","  f = f.transpose().reset_index().drop(['index'], axis=1)\n","  return f.sort_values(0, axis=0, ascending=False, inplace=False, kind='quicksort', na_position='last')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vxYOQGtC8fLZ","colab_type":"code","colab":{}},"source":["#parse individual lesson\n","#total words of JP in the course\n","import pysrt\n","import pandas as pd\n","\n","total_subs = list()\n","for text_n in range(1, 15):\n","  subs = pysrt.open('/content/drive/My Drive/Colab Notebooks/Projects/20200729_NLP_frequency/JP_'+str(text_n)+'.srt')\n","  top_words = top_frequent(subs.text, 12) #n_words qui, metto 12 perch√® le prime 2 si eliminano\n","  df = pd.DataFrame([top_words[x][0] for x in range(10)])\n","  total_subs.append(df)\n","\n","X = pd.concat(total_subs, axis=1)\n","X = X.transpose()\n","X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SpijsmAL_xKW","colab_type":"code","colab":{}},"source":["find_common_factors(X).reset_index().drop(['index'], axis=1)"],"execution_count":null,"outputs":[]}]}